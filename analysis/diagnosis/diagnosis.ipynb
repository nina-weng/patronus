{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../patronus/')\n",
    "from global_config import * # load REPO_HOME_DIR, DATASET_DIR\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from train.utils import load_patronus_unet_model\n",
    "from models.diffusion import SimpleDiffusion\n",
    "from train.dataloader import get_dataloader\n",
    "from analysis.analysis_utils import get_samples_from_loader, vis_samples\n",
    "from analysis.interpretability.visualize_prototype import plot_vis_p,get_most_activated_patch_for_one\n",
    "from train.dataloader import inverse_transform\n",
    "from train.dataloader import get_dataloader,get_dataloader_pact\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis\n",
    "## 1 - Load the model for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'CelebA_hair_smile'\n",
    "version_num = 2\n",
    "print(f'Selecting dataset: {ds} version {version_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load the patronus model -----\n",
    "print('*'*30 + 'Load model' + '*'*30)\n",
    "model, patronus_config_set = load_patronus_unet_model(ds_name=ds, \n",
    "                                                    version_num=version_num,\n",
    "                            )\n",
    "\n",
    " # ---- Load the diffusion schduler ----\n",
    "sd = SimpleDiffusion(\n",
    "        num_diffusion_timesteps = patronus_config_set['TrainingConfig']['TIMESTEPS'],\n",
    "        img_shape               = patronus_config_set['TrainingConfig']['IMG_SHAPE'],\n",
    "        device                  = patronus_config_set['BaseConfig']['DEVICE'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Save the latent and list the most relevent prototype given attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load the patronus model -----\n",
    "print('*'*30 + 'Load model' + '*'*30)\n",
    "model, patronus_config_set = load_patronus_unet_model(ds_name=ds, \n",
    "                                                    version_num=version_num,\n",
    "                            )\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# if the latent npz file exists, load it\n",
    "latent_path = os.path.join(REPO_HOME_DIR, f'records/save_latents/{ds}-{version_num}/{ds}_{version_num}_latent.npz')\n",
    "if os.path.exists(latent_path):\n",
    "    print(f'Latent representation already exists at {latent_path}.')\n",
    "else:\n",
    "    print(f'Latent representation does not exist at {latent_path}. Retrive and save it.')\n",
    "\n",
    "    # ---- Get the prototype encoder -----\n",
    "    prototype_encoder = model.proactBlock\n",
    "    prototype_encoder.eval()\n",
    "    prototype_encoder.to(device)\n",
    "\n",
    "    # get the training data (prototype activations)\n",
    "\n",
    "    # get the prototype activation for the training data and wrap it as dataloader\n",
    "    dataloader_train_pact = get_dataloader_pact(dataset_name=f'{ds}-train',\n",
    "        batch_size=128,\n",
    "        pact_encoder=prototype_encoder,\n",
    "        device=device,\n",
    "        shuffle = False,  # to not need to shuffle here\n",
    "    )\n",
    "\n",
    "    print(f'{len(dataloader_train_pact)=}')\n",
    "\n",
    "    # get the latent representation for the training data\n",
    "    all_pact_train = []\n",
    "    all_attr = []\n",
    "    for i, (x, (extra_info)) in tqdm(enumerate(dataloader_train_pact)):\n",
    "        pact_train_batch = x\n",
    "        pact_train_batch = pact_train_batch.view(pact_train_batch.shape[0], -1)\n",
    "        pact_train_batch = pact_train_batch.cpu().detach().numpy()  # Convert to NumPy\n",
    "        \n",
    "        # Process attributes\n",
    "        label = extra_info[0]\n",
    "        # print(f'{label=}')\n",
    "        if 'CelebA' in ds or 'ffhq256' in ds or 'CHEXPERT' in ds:\n",
    "            label_stacked = torch.stack(label, dim=1)  # Shape [batch_size, num_attributes]\n",
    "        else:\n",
    "            label_stacked = label\n",
    "        all_attr_batch = label_stacked.cpu().detach().numpy()  # Convert to NumPy\n",
    "            # Accumulate results\n",
    "        all_pact_train.append(pact_train_batch)\n",
    "        all_attr.append(all_attr_batch)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1} batches...\")    \n",
    "            \n",
    "    all_pact_train = np.concatenate(all_pact_train, axis=0)  # Shape: [total_samples, feature_dim]\n",
    "    all_attr = np.concatenate(all_attr, axis=0)  # Shape: [total_samples, num_attributes]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'{all_pact_train.shape=}')\n",
    "    print(f'{all_attr.shape=}')\n",
    "\n",
    "\n",
    "\n",
    "    # save it to npz file\n",
    "    save_dir = REPO_HOME_DIR + f'records/save_latents/{ds}-{version_num}/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.savez(save_dir+\"{}_{}_latent\".format(ds,version_num), all_a = all_pact_train, all_attr = all_attr)\n",
    "    print(f'Saved latent representation of {ds} - version {version_num} to {save_dir}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.evaluation.p_quality_tool import eval_disentanglement\n",
    "auroc_score_all, y_names = eval_disentanglement(ds, version_num, return_auroc = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_hair_index = y_names.index('Black_Hair')\n",
    "brown_hair_index = y_names.index('Brown_Hair')\n",
    "blonde_hair_index = y_names.index('Blond_Hair')\n",
    "smiling_index = y_names.index('Smiling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the most high 10 value of each index\n",
    "hair_names = ['Black Hair', 'Brown Hair', 'Blonde Hair','Smiling']\n",
    "auroc_result = auroc_score_all[0] # choose the first fold -- the result usually are not very different\n",
    "top10_hair_smile_records = {}\n",
    "for i,this_hair_index in enumerate([black_hair_index, brown_hair_index, blonde_hair_index,smiling_index]):\n",
    "    print(hair_names[i])\n",
    "    this_hair_auroc_result = auroc_result[this_hair_index,:]\n",
    "    top_10_indices = np.argsort(this_hair_auroc_result)[-10:][::-1].tolist()  # Get the indices of the top 10 highest values\n",
    "\n",
    "    # Get the top 10 highest values\n",
    "    top_10_values = this_hair_auroc_result[top_10_indices]\n",
    "    str_index = [str(i) for i in top_10_indices]\n",
    "\n",
    "    print(\"Top 10 values:\", top_10_values)\n",
    "    print(\"Indices of top 10 values:\", ','.join(str_index))\n",
    "\n",
    "    top10_hair_smile_records[hair_names[i]] = top_10_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - visulize the selected prototypes accordingly\n",
    "### 3.1 - select some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = get_dataloader(\n",
    "        dataset_name=f'{ds}-test',\n",
    "        batch_size=128,\n",
    "        device='cpu',\n",
    "        shuffle = True, # test set should not be shuffled\n",
    ")\n",
    "\n",
    "max_pact = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sample_id = ['198406.jpg','184336.jpg','200877.jpg','196817.jpg','193017.jpg','200659.jpg','194489.jpg','183418.jpg'] # hair and smile\n",
    "# selected_sample_id = ['183106.jpg','187412.jpg','186898.jpg']\n",
    "num_selected_sample = len(selected_sample_id)\n",
    "# find the selected sample by their id in the test set\n",
    "\n",
    "\n",
    "# Create a mapping of IDs to indices for easy lookup\n",
    "id_to_index = {img_id: idx for idx, img_id in enumerate(selected_sample_id)}\n",
    "\n",
    "# Initialize an empty tensor for selected images (assuming all images have the same shape)\n",
    "example_img_shape = next(iter(dataloader_test))[0][0].shape  # Get the shape of a single image\n",
    "selected_img = torch.zeros((num_selected_sample, *example_img_shape))  # Empty tensor to store selected images\n",
    "\n",
    "# Track which IDs have been matched\n",
    "found_ids = set()\n",
    "\n",
    "# Loop through the test dataloader\n",
    "for b_image, extra_info in tqdm(dataloader_test):\n",
    "    b_img_id = extra_info[1]  # Assuming this contains the image IDs\n",
    "    # Find indices of matching IDs in the batch\n",
    "    matching_indices = [i for i, img_id in enumerate(b_img_id) if img_id in id_to_index]\n",
    "    \n",
    "    if matching_indices:  # Only process if there are matches\n",
    "        for i in matching_indices:\n",
    "            idx = id_to_index[b_img_id[i]]  # Find the correct index in `selected_img`\n",
    "            if b_img_id[i] not in found_ids:  # Check if this ID is already processed\n",
    "                selected_img[idx] = b_image[i]  # Place the image in the correct position\n",
    "                found_ids.add(b_img_id[i])  # Mark the ID as found\n",
    "        print(f'Found selected samples: {[b_img_id[i] for i in matching_indices]}')\n",
    "    \n",
    "    if len(found_ids) == num_selected_sample:  # Stop only when all IDs are found\n",
    "        break\n",
    "\n",
    "print(selected_img.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the selected samples\n",
    "fig, axes = plt.subplots(1,num_selected_sample,figsize=(num_selected_sample*4,4))\n",
    "for i in range(num_selected_sample):\n",
    "    this_img = selected_img[i]\n",
    "    # print(this_img.shape)\n",
    "    axes[i].imshow(np.transpose(inverse_transform(this_img).type(torch.uint8).cpu().squeeze().numpy(), (1, 2, 0)))\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'{selected_sample_id[i]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pact = model.proactBlock(selected_img.to(device))\n",
    "print(selected_pact.shape)  \n",
    "\n",
    "selected_xT = sd.reverse_sample_loop(model,selected_img.to(device), model_kwargs={'given_cond_vector':selected_pact})['sample']\n",
    "print(selected_xT.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - choose the prototypes and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_p_ind =  top10_hair_smile_records['Smiling'][:2] +  top10_hair_smile_records['Blonde Hair'][:2] + top10_hair_smile_records['Brown Hair'][:2] + top10_hair_smile_records['Black Hair'][:2] \n",
    "print(f'selected p index: {selected_p_ind}')\n",
    "\n",
    "# for each selected p, maximum it's possible value, and then generate the image\n",
    "highest_activated_record = {}\n",
    "enhanced_pact_chosen_p_all = []\n",
    "\n",
    "for real_i,i_p in enumerate(selected_p_ind):\n",
    "    enhanced_pact_chosen_p = selected_pact.clone()\n",
    "    enhanced_pact_chosen_p[:,i_p] = max_pact\n",
    "    enhanced_pact_chosen_p_all.append(enhanced_pact_chosen_p)\n",
    "\n",
    "enhanced_pact_chosen_p_all = torch.cat(enhanced_pact_chosen_p_all,dim=0)\n",
    "\n",
    "random_pick_img_all = selected_img.repeat(len(selected_p_ind),1,1,1)\n",
    "random_pick_xT_all = selected_xT.repeat(len(selected_p_ind),1,1,1)\n",
    "\n",
    "this_x_0_enhanced_all = sd.sample(model,\n",
    "                                shape=random_pick_img_all.shape ,\n",
    "                                noise=random_pick_xT_all,\n",
    "                                progress=True,\n",
    "                                model_kwargs={'given_cond_vector':enhanced_pact_chosen_p_all},\n",
    "                                num_samples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "num_selected_p =len(selected_p_ind)\n",
    "for real_i,i_p in enumerate(selected_p_ind):\n",
    "    this_x_0_enhanced = this_x_0_enhanced_all[real_i*num_selected_sample:(real_i+1)*num_selected_sample]\n",
    "    this_p_act_all = model.proactBlock(this_x_0_enhanced)\n",
    "    this_p_act = this_p_act_all[:,i_p]\n",
    "\n",
    "    # Another way: do abstract before and after enhance p, and use the one that change the most\n",
    "    this_p_act_all_original = model.proactBlock(selected_img.to(device))\n",
    "    this_p_act_ori = this_p_act_all_original[:,i_p]\n",
    "\n",
    "    # get the most activated patch for this image\n",
    "    most_activated_patches_this =[]\n",
    "    bounding_boxes_this = []\n",
    "    for i in range(num_selected_sample):\n",
    "        most_activated_patches_this_tmp,bounding_boxes_this_tmp = get_most_activated_patch_for_one(this_x_0_enhanced[i].unsqueeze(0),\n",
    "                                                                 [i_p],\n",
    "                                                                 model)\n",
    "        most_activated_patches_this.append(most_activated_patches_this_tmp[0])\n",
    "        bounding_boxes_this.append(bounding_boxes_this_tmp[0])\n",
    "\n",
    "    this_p_act_nor_softmax = torch.nn.functional.softmax(this_p_act_all, dim=1)\n",
    "\n",
    "    records[i_p] = {'ori_img': selected_img,\n",
    "                                     'enhanced_img':this_x_0_enhanced,\n",
    "                                     'enhanced_patch':most_activated_patches_this,\n",
    "                                     'enhanced_b_box':bounding_boxes_this,\n",
    "                                     'enhanced_p_act':this_p_act,\n",
    "                                     'enhanced_p_act_nor':None,\n",
    "                                     'ori_p_act':this_p_act_ori,\n",
    "                                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 15\n",
    "def plot(records,\n",
    "             selected_p_ind,\n",
    "             save_pth=None):\n",
    "\n",
    "    fig, ax = plt.subplots(num_selected_sample, len(selected_p_ind) * 2 + 1, \n",
    "                           figsize=(len(selected_p_ind) * 3, num_selected_sample * 1.5), dpi=100)\n",
    "\n",
    "    col_ind = 0  # Column index starts at 0\n",
    "    ori_img = records[selected_p_ind[0]]['ori_img']\n",
    "\n",
    "    # Plot original images in the first column\n",
    "    for j in range(num_selected_sample):\n",
    "        this_ori_img = ori_img[j]\n",
    "        ax[j, col_ind].imshow(np.transpose(inverse_transform(this_ori_img).type(torch.uint8).cpu().squeeze().numpy(), (1, 2, 0)))\n",
    "        ax[j, col_ind].axis('off')\n",
    "        if j == 0:\n",
    "            ax[j, col_ind].set_title(f\"x_0\", fontsize=fontsize, pad=10)\n",
    "    col_ind += 1\n",
    "\n",
    "    # Plot enhanced patches, images, and bounding boxes\n",
    "    for i_p in selected_p_ind:\n",
    "        patch = records[i_p]['enhanced_patch']\n",
    "        g_img = records[i_p]['enhanced_img']\n",
    "        bounding_boxes = records[i_p]['enhanced_b_box']\n",
    "\n",
    "        # Plot the enhanced images with bounding boxes\n",
    "        for j in range(num_selected_sample):\n",
    "            img = g_img[j]\n",
    "            ax[j, col_ind].imshow(np.transpose(inverse_transform(img).type(torch.uint8).cpu().squeeze().numpy(), (1, 2, 0)))\n",
    "            b_box = bounding_boxes[j]\n",
    "\n",
    "            # Draw bounding box with red lines\n",
    "            ax[j, col_ind].plot([b_box[0], b_box[2]], [b_box[1], b_box[1]], 'r')\n",
    "            ax[j, col_ind].plot([b_box[0], b_box[2]], [b_box[3], b_box[3]], 'r')\n",
    "            ax[j, col_ind].plot([b_box[0], b_box[0]], [b_box[1], b_box[3]], 'r')\n",
    "            ax[j, col_ind].plot([b_box[2], b_box[2]], [b_box[1], b_box[3]], 'r')\n",
    "\n",
    "            ax[j, col_ind].set_xlim([0, img.shape[1]])\n",
    "            ax[j, col_ind].set_ylim([img.shape[2], 0])\n",
    "            ax[j, col_ind].axis('off')\n",
    "            ax[j, col_ind].tick_params(labelbottom=False)\n",
    "\n",
    "            if j == 0:\n",
    "                ax[j, col_ind].set_title(f\"\\hat_x(p'_{i_p})\", fontsize=fontsize, pad=10)\n",
    "        col_ind += 1\n",
    "\n",
    "        # Plot the patches\n",
    "        for j in range(num_selected_sample):\n",
    "            p = patch[j].squeeze(0)\n",
    "            ax[j, col_ind].imshow(np.transpose(inverse_transform(p).type(torch.uint8).cpu().squeeze().numpy(), (1, 2, 0)))\n",
    "            ax[j, col_ind].axis('off')\n",
    "            if j == 0:\n",
    "                ax[j, col_ind].set_title(f\"p_{i_p}\", fontsize=fontsize, pad=10)\n",
    "        col_ind += 1\n",
    "\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.0)\n",
    "    if save_pth is not None:\n",
    "        plt.savefig(save_pth, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_pth = './plot_diagnosis.pdf'\n",
    "save_pth = None\n",
    "plot(records,\n",
    "     selected_p_ind,\n",
    "     save_pth=save_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absorb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
