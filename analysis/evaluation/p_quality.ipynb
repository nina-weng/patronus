{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../patronus/')\n",
    "# print(sys.path)\n",
    "from global_config import * # load REPO_HOME_DIR, DATASET_DIR\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "from train.utils import load_patronus_unet_model\n",
    "from models.diffusion import SimpleDiffusion\n",
    "from train.dataloader import get_dataloader,get_dataloader_pact\n",
    "from analysis.analysis_utils import get_samples_from_loader, vis_samples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype quality\n",
    "We followed InfoDiffusion for accessing prototype (latent) quality, which contains of two parts:  \n",
    "\n",
    "a) *Prototype capability in semantic representation*:  \n",
    "Meaning the performance of using latents predicting known semantic attributes.   \n",
    "b) *Prototype disentanglement*:  \n",
    "Meaning the performance of using signle dimension of latents predicting known semantic attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Save the latent of certain train version (it might take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting dataset: CelebA version 4\n"
     ]
    }
   ],
   "source": [
    "ds = 'CelebA'\n",
    "version_num = 4\n",
    "print(f'Selecting dataset: {ds} version {version_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load the patronus model -----\n",
    "print('*'*30 + 'Load model' + '*'*30)\n",
    "model, patronus_config_set = load_patronus_unet_model(ds_name=ds, \n",
    "                                                    version_num=version_num,\n",
    "                            )\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# if the latent npz file exists, load it\n",
    "latent_path = os.path.join(REPO_HOME_DIR, f'records/save_latents/{ds}-{version_num}/{ds}_{version_num}_latent.npz')\n",
    "if os.path.exists(latent_path):\n",
    "    print(f'Latent representation already exists at {latent_path}.')\n",
    "else:\n",
    "    print(f'Latent representation does not exist at {latent_path}. Retrive and save it.')\n",
    "\n",
    "    # ---- Get the prototype encoder -----\n",
    "    prototype_encoder = model.proactBlock\n",
    "    prototype_encoder.eval()\n",
    "    prototype_encoder.to(device)\n",
    "\n",
    "    # get the training data (prototype activations)\n",
    "\n",
    "    # get the prototype activation for the training data and wrap it as dataloader\n",
    "    dataloader_train_pact = get_dataloader_pact(dataset_name=f'{ds}-train',\n",
    "        batch_size=128,\n",
    "        pact_encoder=prototype_encoder,\n",
    "        device=device,\n",
    "        shuffle = False,  # to not need to shuffle here\n",
    "    )\n",
    "\n",
    "    print(f'{len(dataloader_train_pact)=}')\n",
    "\n",
    "    # get the latent representation for the training data\n",
    "    all_pact_train = []\n",
    "    all_attr = []\n",
    "    for i, (x, (extra_info)) in tqdm(enumerate(dataloader_train_pact)):\n",
    "        pact_train_batch = x\n",
    "        pact_train_batch = pact_train_batch.view(pact_train_batch.shape[0], -1)\n",
    "        pact_train_batch = pact_train_batch.cpu().detach().numpy()  # Convert to NumPy\n",
    "        \n",
    "        # Process attributes\n",
    "        label = extra_info[0]\n",
    "        # print(f'{label=}')\n",
    "        if 'CelebA' in ds or 'ffhq256' in ds or 'CHEXPERT' in ds:\n",
    "            label_stacked = torch.stack(label, dim=1)  # Shape [batch_size, num_attributes]\n",
    "        else:\n",
    "            label_stacked = label\n",
    "        all_attr_batch = label_stacked.cpu().detach().numpy()  # Convert to NumPy\n",
    "            # Accumulate results\n",
    "        all_pact_train.append(pact_train_batch)\n",
    "        all_attr.append(all_attr_batch)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1} batches...\")    \n",
    "            \n",
    "    all_pact_train = np.concatenate(all_pact_train, axis=0)  # Shape: [total_samples, feature_dim]\n",
    "    all_attr = np.concatenate(all_attr, axis=0)  # Shape: [total_samples, num_attributes]\n",
    "\n",
    "\n",
    "\n",
    "    print(f'{all_pact_train.shape=}')\n",
    "    print(f'{all_attr.shape=}')\n",
    "\n",
    "\n",
    "\n",
    "    # save it to npz file\n",
    "    save_dir = REPO_HOME_DIR + f'records/save_latents/{ds}-{version_num}/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.savez(save_dir+\"{}_{}_latent\".format(ds,version_num), all_a = all_pact_train, all_attr = all_attr)\n",
    "    print(f'Saved latent representation of {ds} - version {version_num} to {save_dir}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Run evaluation\n",
    "Notice that disentanglement part are only available for CelebA and CheXpert dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_quality_tool import eval_disentanglement\n",
    "eval_disentanglement(ds_name=ds, version_num=version_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absorb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
